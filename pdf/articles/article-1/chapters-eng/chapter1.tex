\chapter{Review of terms and existing solutions}

\startrelatedwork

For now the automation of different problems like community search in social networks is very relevant and progress from day to day. There are different methods for finding communities in the whole network \cite{Newman04, Newman06, Fortunato10, Cui13} and also for finding a dense community containing all selected vertices in the network \cite{Faloutsos06, Wiener15, Huang15, Barbieri15}. There are even algorithms for splitting selected vertices into several communities \cite{Akoglu13} (DOT2DOT algorithm). However, almost all of these algorithms don't support noise in the query and find non-optimal subgraphs on such queries. Precisely this problem we are going to solve in this work, suggesting the new algorithm that will effectively find the needed subgraph even if there is some noise in the query.

\section{Terms and definitions}

\subsection{Graph terms}

In this chapter we will write down all non-common terms and definitions that may be helpful for the further reading.


Under \textbf{$N_G(v)$} we will understand the set of the neighbors of vertex $v$ in graph $G$, i.e. the set of the vertices that are directly connected to $v$ by edge: $N_G(v) = \{u | (v, u) \in E(G)\}$. If graph $G$ can be obviously recognized from the context, we can write just $N(v)$.

\textbf{$G[V]$} is called \textbf{originated subgraph} of the graph $G$ by the set of vertices $V$ if $G[V] := (V, E[G, V])$, where $E[G, V]$~--- the subset of the set of edges of $G$, both ends of which are contained in $V$, i.e. $E[G, V] = E(G) \cap (V \times V)$.

\textbf{k-truss} of the graph $G$ is the subgraph $G' \subseteq G$ containing the maximal possible number of vertices, such that for each edge $(v, u)$ the number of vertices $w$, such that edges $(w, v)$ and $(w, u)$ exists in $G'$ is at least $k$. In other words, $k-truss$ is the maximal by size subgraph $G'$, for each of which edge $(v, u)$, the $|N_{G'}(v) \cap N_{G'}(u)| >= k$ is true.

\textbf{k-core} of the graph $G$ is called the maximal by the number of vertices subgraph $G' \subseteq G$, so that the degree of each of its vertices is at least $k$. For the fixed $k$, by $C_k$ we will denote \textit{$k$-core}, namely the set of the connected components of which it consists. So, $C_k = \{H_i\}$, where $H_i$ is the $i$-th connected component, where the degree of each vertex is at least $k$. The number $k$ we will call the \textbf{order} of \textit{k-core}.

By \boldmath$\mu(G)$\unboldmath we will denote the minimal degree of the vertices $G$, i.e. $\mu(G) = \min_{v \in V(G)} deg(v)$.

\textbf{Core decomposition} is the set of \textit{k-core} for all possible $k$: $C = \{C_k\}_{k=1}^{k=k^*}$. We also need to clarify that from the definition of the \textit{k-core} you can see that $C_1 \supseteq C_2 \supseteq C_3 \ldots \supseteq C_{k^*}$ (where $k^*$ is the maximal possible $k$ in core decomposition).

\textbf{Ð¡ore index} for the vertex $v$ is called the minimal by the size \textit{k-core} which includes $v$, i.e. \textit{k-core} with the maximal $k$: $c(v) = \max(k \in [0..k^*] | v \in C_k)$.

\textbf{$\gamma$-quasi-clique} of the graph $G$ is called any such subgraph $G' \subseteq G$ that it is <<dense enough>>, i.e. $\frac{2 \cdot |E(G')|}{|V(G')| \cdot (|V(G')| - 1)} \ge \gamma$.

\subsection{Social networks}

\textbf{The community} or \textbf{The community in social network} is called the set of vertices of the social network $G$, where all vertices are united by some property or attribute. For example, <<the community of rock lovers>> or <<the community of Apple shareholders>>.

\textbf{The social clique} or \textbf{clique} we will call the set of people in social network, where everyone "knows" (i.e. is connected by edge) each other, in other words when between any pair of distinct people there is an edge in social network.

\textbf{Social pseudoclique} or \textbf{pseudoclique} we will call the set of people, where it is not required that each pair of distinct people is connected by edge, but this set is still densely connected. The estimation, how dense the pseudoclique is connected depends on the type of the pseudoclique and will be discussed later in the work, but in all definitions the biggest role plays the number of edges in subgraph in comparison with the number of pairs of vertices ($\frac{2 \cdot |E(G)|}{|V(G)| \cdot (|V(G)| - 1)}$).

\textbf{Free-rider effect} is called the effect, appearing during the obtaining the answer for the given problem (finding the dense community in social network), when the final or intermediate answer contains unnecessary subgraphs~--- subgraphs which can be deleted without violation the optimality of the answer. Thereby, we can make the answer smaller which is one of ours main goals.

\subsection{Useful abbreviations}

\textbf{RW}~--- Random Walks. The main idea of this method is based on moving from one vertex to the neighbor one with probability proportional to the edge weight.

\textbf{RWR}~--- Random Walks with Restarts. The idea is similar to the RW, but in this case the probability to move to the initial vertex (from which we've started) from the current one exists as well.

\textbf{Smart-ST}~--- Smart Spanning Trees. The heuristic for Steiner Tree problem, which is used in Gionis et al. article \cite{Gionis15}.

\textbf{CSP}~--- Community Search Problem. This is the problem for finding the community in the social network which contains all the selected vertices.

\textbf{NCSP}~--- Noising Community Search Problem. This is the problem for finding the community in the social network which contains most of the selected vertices, but not necessary all (not including the noise).

\section{Overview}

The problem that we're analyzing in the work is formulated as follows: given a graph $G$ and a set of selected vertices $Q \subset V(G)$, the goal is to solve community search problem~--- to find the community which contains most of the vertices from $Q$, but not necessary all of them. Sometimes we will call vertices from $Q$ <<query vertices>>, <<query>> or <<vertices from query>>.

\subsection{Initial solutions}

\begin{enumerate}
  \item The community search problem by the given selected vertices is researched during a long time. Even in 2004 Faloutsos et al. \cite{Faloutsos04} suggested the algorithms for finding the dense community by $2$ selected vertices in network ($|Q| = 2$). The algorithm shows that metrics like <<the shortest path>> and <<max flow>> between two given vertices are not optimal. Instead of them, the initial graph is considered as the electric network and <<the current delivered between vertices>> metric is used~--- setting voltage \texttt{+1} on the first vertex-query and \texttt{0} on the second one, we find subgraph which delivers the maximal current between vertices from query. The provided metrics works only for $|Q| = 2$, but this algorithm was the foundation in research of the community search problem. After that many authors were working on optimization for this article and were quite successful.

  \item Authors of the second article that was taken for consideration \textbf{Faloutsos06} suggest the metric function based on \textit{random walks with restarts} (RWR) used on the weighted graph. They consider $r(i, j)$~--- the probability that starting in vertex $i$-th query vertex $q_i$ we will end in vertex $j$ using RWR, where on each step we move to the neighbor by edge vertex with probability proportional the edge weight. Also we introduce $r(Q, j)$ which is equal to the sum of $r(i, j)$ for each query vertices: $r(Q, j) = \sum_{i = 1}^{i = |Q|}\; r(i, j)$. The dense metric is considered as $g(H) = \sum_{j \in H}\; r(Q, j)$. This method has shown quite good results compared to the previous article, because expanded the number of query vertices from $2$ to any number from $2$ to $|V(G)|$. Also, it introduced new ability of finding the subgraph containing not all vertices, but at least $k$ of them ($k$ is a parameter which is given as input). This operation was called $K\_softAND$ and was successfully implemented in the article. Further algorithms were expanding this idea, were applying other metrics and improved the results of this algorithm, but the problem of finding the community containing not necessary all query vertices, but only a part of them (our problem), almost wasn't optimized.

  \item Authors of the third article \cite{Wiener15} suggest to use \textit{Wiener index} as the metric for subgraph density. This metric is equal to the pairwise sum of the shortest distances between vertices from the query. Authors are trying to solve the issue of obtaining too large graph as the result of processing the query if query vertices are placed in several communities and are weakly connected between each other. To solve this issue, authors suggest to add some <<important vertices>> to the query which will connect communities, even if not very dense. Results has shown that this method works several times better than previous methods \cite{Faloutsos06, Sozio10} and almost the same as the methods that based on \textit{Steiner tree problem}. Unfortunately, the article doesn't consider more late methods based on \textit{Steiner tree problem} which significantly improved the old results, which make this method less priority comparing to them.
  
\end{enumerate}

\subsection{Finding optimal pseudocliques}

The most part of all algorithms for solving the described problem are the algorithms based on the finding optimal pseudocliques with some additional heuristics. There are a lot of different pseudocliques the were considered in different articles: for example, \textit{k-core} \cite{Barbieri15}, \textit{k-truss} \cite{Huang15}, \textit{$\gamma$-quasi-clique} \cite{Zhu11} or just algorithms that maximizes the edge density in the resulting subgraph \cite{Wu15} which is almost a definition of a pseudoclique. For each of these pseudocliques the algorithms are evolving and becoming better, optimizing the previous results using new heuristics. Comparing the results of the algorithms that use different pseudocliques is quite hard and unlikely will give visible results because of the difference of the metrics that are being optimized~--- the result strongly depend on the initial graph and the queries on it. In some cases one pseudoclique will obtain results better than others, but in other cases it will work worse, so actually it's worth to compare some common performance metrics, but unfortunately it doesn't give us the whole understanding of the optimality or non-optimality of the algorithms.

Let's consider several newest algorithms for the most popular pseudocliques:

\begin{enumerate}
  \item X. Huang et al. \cite{Huang15} choose \textit{k-truss} pseudoclique. However, just finding the optimal \textit{k-truss} (i.e. \textit{k-truss} with the maximal $k$ containing all query vertices in it) is not an optimal solution, and also it is an already solved problem (even with polynomial solution). That's why the authors of the article suggest to find \textit{k-truss} with maximal $k$ and minimal subgraph diameter, which, as they show in their article, is a NP-hard problem. However, this idea pretends to show good results, so authors made a research trying to understand how close the answer found by polynomial time may be to the theoretically optimal answer. It turned out that this problem couldn't be solved with accuracy better than in $(2 - \varepsilon)$ times worse for each $\varepsilon > 0$ (under the accuracy we understand the length of the diameter in the final answer). However, authors suggested the heuristic algorithm which in the worst case makes exactly $2$ times error, which shows that their algorithm is optimal for the provided problem. The algorithm is based on building the supposed maximal $\textit{k-truss}$ with the followed iterative deleting vertices which doesn't make the answer worse and make the diameter smaller. The results obtained in this article are really good comparing to the previous articles \cite{Sozio10, Wu15}, however, even despite of proved optimality for the provided algorithm, it is not optimal for the initial problem (finding the dense subgraph by the given query vertices), because authors found the optimal solution only for the problem provided \textit{by themselves}.

  
  \item N. Barbieri et al. \cite{Barbieri15} is using \textit{k-core} pseudoclique. However, even here simple finding the optimal \textit{k-core} (i.e. \textit{k-core} with maximal $k$ which contains all query vertices in it) is not an optimal solution, and also this problem is already solved by polynomial time \cite{Sozio10}. That's why the authors of the article apply some heuristics targeted for minimizing the size of the resulting subgraph without loosing its optimality. These heuristics allow to reduce the problem to finding the answer in component $H^* \subset G$, and besides it is guaranteed that all possible optimal answers for the initial problem are lying in $H^*$. After that authors bring some heuristics for minimizing the obtained subgraph $H^*$. The main statement described by the authors is not new, but it looks quite interesting for the further researches because it adds more information to the initial problem without loosing any solutions. The results of this article shows that provided method really works better and faster than previous ones \cite{Sozio10, Cui14}. Based on all the above information, it was decided to take this article as the baseline and try to improve it, especially to expand it to our problem (finding the community containing not necessary all selected vertices).
\end{enumerate}

\subsection{Other methods}

As we already saw earlier, optimizing functions may be quite different. In the previous part we were considering pseudocliques, and here we're going to consider several other popular optimizing functions.

\begin{enumerate}
  \item L. Akoglu et al. \cite{Akoglu13} slightly change the initial problem~--- they try to find subgraph that unites not all the vertices in the query, but their groups. Actually the idea is based on splitting the query into groups and building the answer for each of the groups separately, so that in each groups the vertices are densely connects and are united by some common property. But between each other the groups may be connected not very densely. This corresponds to the splitting the query into several communities. The results of the article has shown that this method solves the problem provided by authors quite good, but however as we said before, this problem differs from ours and it's hard to compare it's results with ours. However, we still are going to compare this solution with ours, because this solution supports finding community not for all selected vertices. 

  \item A. Gionis et al. \cite{Gionis15} in their article consider \textit{linear local discrepancy} metric which is equal to the weighted difference of the number of query vertices in the resulting subgraph and the number of remaining vertices. More formally, $g(C) = \alpha p_C - n_C$, where $p_C = |Q \cap V(C)|$ and $n_C = |V(C) \setminus Q|$. The algorithm that maximized this function based on the \textit{Steiner tree problem} and \textit{Smart-ST}s. The distinguishing feature of this algorithm is the ability of solving the problem using \textit{local access model}, i.e. the model where we don't know the whole graph (or it is too big to save it in RAM), and API allows us only to make queries for accessing all vertex neighbors~--- $get-neighbors$ method. This model allows to solve the problem optimally even on very big social networks, such as \textit{Twitter} or \textit{Facebook}. Based on the definition, the resulting subgraph may not contain all vertices from query, which coincides with our research. Unfortunately, the provided metric doesn't fit our problem very good~--- it doesn't take into consideration edge density of the resulting subgraph, it looks only the ratio of the vertices. Also it's possible that the answer will contain too few vertices from the initial query~--- it also doesn't work for us.
\end{enumerate}

\section{Final requirements for our work}

Let's sum up everything described above. Most of the current solutions solves CSP quite optimal~--- each of the solutions uses it's own metric and obtains quite good results. However, as we can see, solutions for NCSP (which includes noise into consideration) are quite rare, despite it is more useful problem in real life. We've noted consideration of NCSP problem in articles C. Faloutsos \& H. Tong \textbf{[3]} and A. Gionis et al. \cite{Gionis15}, however the last problem is not based on solving NCSP (but solves it at the same time). So, the goal of our article will be to build the algorithm that focuses on NCSP solving and obtains better results than the current ones. Here are some requirements for our algorithm:

\begin{itemize}
    \item The algorithm should obtain better results than the current ones \cite{Faloutsos06, Gionis15, Barbieri15};
    \item The algorithm should be quite optimal, ideally not loosing the competition with other algorithms in terms of working time;
    \item It would be an advantage to support backwards compatibility~--- if the user wants to find subgraph that contains \textit{all} query vertices, it should be possible to be done.
\end{itemize}
